#!/usr/bin/env python3
"""trudag_normalized_items_checker

Checks and (optionally) normalizes item frontmatter across `items/` and `.trudag_items/`.

Behaviours implemented (dry-run by default):
- validate frontmatter keys (id, header, text, level, normative)
- ensure references entries have `type` and `path` and that referenced paths exist
- normalize paths (%20 -> space, collapse duplicate `docs/` prefixes)
- for `expectations` and `assertions`: remove `score` key (SME scores should live on leaves)
- for `evidences` and `assumptions` (leaves): check they have `references` or `evidence`
- sync canonical `.trudag_items` -> `items` so both trees match

Usage:
  ./trudag_normalized_items_checker.py [--root PATH] [--apply] [--verbose]

By default performs a dry-run and prints proposed changes.
"""
import argparse
import os
import re
import sys
from pathlib import Path
import yaml
import subprocess

REPO_ROOT = Path(__file__).resolve().parents[2]
DEFAULT_IMPL_ROOT = REPO_ROOT / "docs" / "TSF" / "tsf_implementation"


def split_frontmatter(text: str):
    parts = text.split('---')
    if len(parts) >= 3:
        # parts: ['', '\nkey: val\n', '\nbody...']
        fm = parts[1]
        body = '---'.join(parts[2:]).lstrip('\n')
        return fm, body
    return None, text


def normalize_path(p: str) -> str:
    # replace %20 with space, collapse duplicate docs/ segments, strip trailing spaces
    p = p.replace('%20', ' ')
    p = re.sub(r'(docs/)+', 'docs/', p)
    return p.strip()


def validate_required_fields(fm: dict, path: Path, errors: list):
    # Ensure required frontmatter keys exist and are well formed: id, header, text, normative
    required = ['id', 'header', 'normative']
    for key in required:
        if key not in fm:
            errors.append((path, 'missing_field', key))
    # text is allowed to be in body; if normative True then text must be present
    if fm.get('normative'):
        # if `text` key missing, ensure body non-empty will be checked by caller if needed
        if 'text' not in fm:
            # caller may pass body separately; signal missing text key
            errors.append((path, 'missing_field', 'text'))


def check_score_is_float(fm: dict, path: Path, errors: list):
    """Verify that any `score` present is a float (or dict of floats).

    If a numeric score is present but not a Python float (for example an int),
    record an error so the user can correct it. This function does not modify
    files.
    """
    if 'score' not in fm:
        return
    s = fm['score']
    # scalar numeric (int) -> report
    if isinstance(s, int):
        errors.append((path, 'score_not_float', s))
        return
    # valid float
    if isinstance(s, float):
        return
    # mapping of named scores -> ensure all values are floats
    if isinstance(s, dict):
        bad = {}
        for k, v in s.items():
            if isinstance(v, int):
                bad[k] = v
            elif isinstance(v, float):
                continue
            else:
                # non-numeric values are also flagged
                bad[k] = v
        if bad:
            errors.append((path, 'score_not_float', bad))
        return
    # other types (string, list, etc.) - flag as non-float
    errors.append((path, 'score_not_float', s))


def convert_scores_to_float(root: Path, apply: bool, changes: list, errors: list):
    """Find `score` entries that are ints (or dict values that are ints) and
    convert them to floats when `apply` is True.

    Records proposed changes in `changes` and any YAML errors in `errors`.
    """
    for sub in ['items', '.trudag_items']:
        base = root / sub
        if not base.exists():
            continue
        for p in sorted(base.rglob('*.md')):
            # skip possible autogenerated item.md files
            if sub == '.trudag_items' and p.name.lower() == 'item.md':
                continue
            try:
                fm, body = load_markdown(p)
                if isinstance(fm, dict) and fm.get('__yaml_error'):
                    errors.append((p, 'yaml_error', fm.get('__yaml_error')))
                    continue
                if not isinstance(fm, dict):
                    continue
                changed = False
                if 'score' in fm:
                    s = fm['score']
                    # scalar int -> convert to float
                    if isinstance(s, int):
                        if apply:
                            fm['score'] = float(s)
                            write_markdown(p, fm, body)
                            changes.append((p, 'convert_score_int_to_float'))
                        else:
                            changes.append((p, 'would_convert_score_int_to_float'))
                        changed = True
                    # dict of scores
                    elif isinstance(s, dict):
                        newd = {}
                        local_changed = False
                        for k, v in s.items():
                            if isinstance(v, int):
                                newd[k] = float(v)
                                local_changed = True
                            else:
                                newd[k] = v
                        if local_changed:
                            if apply:
                                fm['score'] = newd
                                write_markdown(p, fm, body)
                                changes.append((p, 'convert_score_dict_ints_to_floats'))
                            else:
                                changes.append((p, 'would_convert_score_dict_ints_to_floats'))
                            changed = True
                # also consider nested references in frontmatter (nothing to do here)
            except Exception as e:
                errors.append((p, 'convert_error', str(e)))


def suggest_frontmatter_fixes(root: Path, apply: bool, changes: list, errors: list, interactive: bool=False):
    """Scan markdown frontmatter and propose sensible fixes for missing/incorrect keys.

    Suggestions include: `id`, `level` (from filename L0-n -> 1.n), `header`, `normative` (default False).
    Additionally, when a file explicitly contains `normative: false` the checker now
    suggests flipping it to `true` (this suggestion is reported in dry-run and applied
    when `--apply` is given; `--interactive` will prompt per-file).
    and `text` when `normative` is True. When `apply` is True the suggestions are written.
    If `interactive` is True, prompt for each suggestion before applying.
    """
    for sub in ['items', '.trudag_items']:
        base = root / sub
        if not base.exists():
            continue
        for p in sorted(base.rglob('*.md')):
            if sub == '.trudag_items' and p.name.lower() == 'item.md':
                continue
            try:
                fm, body = load_markdown(p)
                if isinstance(fm, dict) and fm.get('__yaml_error'):
                    errors.append((p, 'yaml_error', fm.get('__yaml_error')))
                    continue
                if not isinstance(fm, dict):
                    fm = {}
                suggestion = {}
                # id
                if 'id' not in fm:
                    suggested_id = p.stem.replace('.md', '')
                    suggestion['id'] = suggested_id
                # level from filename
                m = re.search(r'L0-(\d+)', p.name)
                if m and 'level' not in fm:
                    suggestion['level'] = f"1.{m.group(1)}"
                # header
                if 'header' not in fm:
                    # use first non-empty line of body or filename
                    header = None
                    if body:
                        for line in body.splitlines():
                            s = line.strip()
                            if s:
                                header = s
                                break
                    if not header:
                        header = p.stem.replace('-', ' ')
                    suggestion['header'] = header
                # normative default when missing
                if 'normative' not in fm:
                    suggestion['normative'] = False
                # if normative explicitly set to False, suggest flipping to True
                elif fm.get('normative') is False:
                    # propose flipping existing normative False to True
                    suggestion['normative'] = True
                    # marker to indicate we intend to overwrite existing value
                    suggestion['__flip_normative'] = True
                # text when normative True
                if suggestion.get('normative') or fm.get('normative'):
                    if 'text' not in fm:
                        # propose using first paragraph of body
                        first_para = ''
                        if body:
                            for para in body.split('\n\n'):
                                t = para.strip()
                                if t:
                                    first_para = t
                                    break
                        suggestion['text'] = first_para

                if suggestion:
                    # format human-friendly suggestion
                    changes.append((p, 'would_suggest_frontmatter_fix', suggestion))
                    if apply:
                        # interactive prompt per-file
                        do_apply = True
                        if interactive:
                            # ask user
                            resp = input(f"Apply suggested frontmatter fixes to {p.relative_to(Path.cwd())}? [y/N]: ")
                            do_apply = resp.strip().lower() in ('y', 'yes')
                        if do_apply:
                            # merge suggestions into fm and write
                            for k, v in suggestion.items():
                                # skip internal markers
                                if isinstance(k, str) and k.startswith('__'):
                                    continue
                                # normally only set if not present to avoid clobbering existing values
                                if k not in fm or fm.get(k) in (None, ''):
                                    fm[k] = v
                                else:
                                    # but allow overwriting normative when we explicitly suggested a flip
                                    if k == 'normative' and suggestion.get('__flip_normative'):
                                        fm[k] = v
                            write_markdown(p, fm, body)
                            changes.append((p, 'applied_frontmatter_fix'))
            except Exception as e:
                errors.append((p, 'suggest_fix_error', str(e)))


def enforce_level_matches_filename(fm: dict, path: Path, errors: list):
    # For files named with L0-<n> expect level '1.<n>' (or '1.<n>.<...>')
    m = re.search(r'L0-(\d+)', path.name)
    if not m:
        return
    number = m.group(1)
    expected = f"1.{number}"
    level = fm.get('level')
    if level is None:
        errors.append((path, 'missing_field', 'level'))
        return
    # compare as strings (allow additional sublevels after the expected prefix)
    slevel = str(level)
    if not slevel.startswith(expected):
        errors.append((path, 'level_mismatch', {'expected_prefix': expected, 'found': slevel}))


def is_concrete_ref(ref: dict) -> bool:
    # Heuristic: references that point to repo docs/src/images or image files are 'concrete'
    if not isinstance(ref, dict):
        return False
    path = ref.get('path') or ''
    if not path:
        return False
    lower = path.lower()
    if 'docs/' in lower or 'src/' in lower or '/images/' in lower:
        return True
    if lower.endswith(('.png', '.jpg', '.jpeg', '.svg', '.gif')):
        return True
    return False


def load_markdown(path: Path):
    text = path.read_text(encoding='utf-8')
    fm_text, body = split_frontmatter(text)
    if fm_text is None:
        return {}, body
    try:
        fm = yaml.safe_load(fm_text) or {}
    except Exception as e:
        # return the error in the fm so callers can report and skip
        return {'__yaml_error': str(e)}, body
    return fm, body


def write_markdown(path: Path, fm: dict, body: str):
    content = '---\n' + yaml.safe_dump(fm, sort_keys=False) + '---\n\n' + (body or '')
    path.write_text(content, encoding='utf-8')


def fix_missing_reference_types(root: Path, apply: bool, changes: list, errors: list):
    """Walk `items` and `.trudag_items` and ensure every reference has a `type`.

    Sets `type: url` when `path` starts with http(s), otherwise `type: file`.
    Normalizes `path` entries as well. If `apply` is True, writes changes.
    Records changes in `changes` and parse/write errors in `errors`.
    """
    for sub in ['items', '.trudag_items']:
        base = root / sub
        if not base.exists():
            continue
        for p in sorted(base.rglob('*.md')):
            if sub == '.trudag_items' and p.name.lower() == 'item.md':
                continue
            try:
                fm, body = load_markdown(p)
                if isinstance(fm, dict) and fm.get('__yaml_error'):
                    errors.append((p, 'yaml_error', fm.get('__yaml_error')))
                    continue
                changed = False
                if 'references' in fm and isinstance(fm['references'], list):
                    new_refs = []
                    for ref in fm['references']:
                        if not isinstance(ref, dict):
                            new_refs.append(ref)
                            continue
                        # infer type if missing
                        if 'type' not in ref:
                            pth = str(ref.get('path', '')).strip()
                            if pth.lower().startswith('http'):
                                ref['type'] = 'url'
                            else:
                                ref['type'] = 'file'
                            changed = True
                        # normalize path if present
                        if 'path' in ref:
                            norm = normalize_path(str(ref['path']))
                            if norm != ref['path']:
                                ref['path'] = norm
                                changed = True
                        new_refs.append(ref)
                    if changed and apply:
                        fm['references'] = new_refs
                        write_markdown(p, fm, body)
                        changes.append((p, 'fix_missing_reference_type'))
                    elif changed:
                        changes.append((p, 'would_fix_missing_reference_type'))
            except Exception as e:
                errors.append((p, 'fix_error', str(e)))


def find_items(root: Path):
    items_dir = root / 'items'
    trudag_dir = root / '.trudag_items'
    item_files = []
    # collect from items/
    for sub in ['expectations', 'assertions', 'assumptions', 'evidences']:
        d = items_dir / sub
        if not d.exists():
            continue
        for p in sorted(d.glob('*.md')):
            item_files.append((sub, p, 'items'))
    # collect canonical files from .trudag_items
    for prefix_dir in sorted(trudag_dir.glob('*')):
        if not prefix_dir.is_dir():
            continue
        for uid_dir in sorted(prefix_dir.glob('*')):
            if not uid_dir.is_dir():
                continue
            for p in sorted(uid_dir.glob('*.md')):
                if p.name.lower() == 'item.md':
                    continue
                # infer sub from prefix name
                prefix = prefix_dir.name.lower()
                sub = None
                if 'expect' in prefix:
                    sub = 'expectations'
                elif 'assert' in prefix:
                    sub = 'assertions'
                elif 'assump' in prefix or 'assum' in prefix:
                    sub = 'assumptions'
                elif 'evid' in prefix:
                    sub = 'evidences'
                else:
                    sub = prefix
                item_files.append((sub, p, '.trudag_items'))
    return item_files


def relative_in_repo(path: str, root: Path) -> Path:
    p = Path(path)
    if p.is_absolute():
        return p
    # try to resolve relative to root
    candidate = (root / path).resolve()
    return candidate


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--root', default=str(DEFAULT_IMPL_ROOT))
    ap.add_argument('--apply', action='store_true')
    ap.add_argument('--interactive', action='store_true', help='Prompt before applying suggested fixes')
    ap.add_argument('--verbose', action='store_true')
    args = ap.parse_args()

    root = Path(args.root)
    if not root.exists():
        print('Root not found:', root)
        sys.exit(2)

    changes = []
    errors = []

    # First pass: fix missing reference types across both trees
    # First, convert int scores to floats if requested (or report them)
    convert_scores_to_float(root, args.apply, changes, errors)

    # Suggest frontmatter fixes (and optionally apply)
    suggest_frontmatter_fixes(root, args.apply, changes, errors, interactive=args.interactive)

    # Next pass: fix missing reference types across both trees
    fix_missing_reference_types(root, args.apply, changes, errors)

    # scan items tree (prefer authoritative .trudag_items canonical files when syncing)
    # First, build map of canonical files for easy sync
    canonical_map = {}
    trudag_dir = root / '.trudag_items'
    for prefix_dir in sorted(trudag_dir.glob('*')):
        if not prefix_dir.is_dir():
            continue
        for uid_dir in sorted(prefix_dir.glob('*')):
            if not uid_dir.is_dir():
                continue
            for p in sorted(uid_dir.glob('*.md')):
                if p.name.lower() == 'item.md':
                    continue
                # compute target path under items
                # canonical file like PREFIX-NAME.md -> target NAME with underscores->hyphens
                name = p.name.split('-', 1)[1].replace('_', '-')
                canonical_map[p] = name

    # Now process items/ and .trudag_items
    items_dir = root / 'items'
    # convenience: gather all item files under items/ structured by sub
    items_files = { 'expectations': [], 'assertions': [], 'assumptions': [], 'evidences': [] }
    # walk expectations/assertions to remove score
    for sub in ['expectations', 'assertions']:
        d = items_dir / sub
        if not d.exists():
            continue
        for f in sorted(d.glob('*.md')):
            fm, body = load_markdown(f)
            if isinstance(fm, dict) and fm.get('__yaml_error'):
                errors.append((f, 'yaml_error', fm.get('__yaml_error')))
                continue
            # validate required fields and level
            validate_required_fields(fm, f, errors)
            # verify score fields are floats (report-only)
            check_score_is_float(fm, f, errors)
            enforce_level_matches_filename(fm, f, errors)
            if 'score' in fm:
                changes.append((f, 'remove_score'))
                if args.apply:
                    fm.pop('score', None)
                    write_markdown(f, fm, body)
            items_files[sub].append((f, fm, body))

    # process leaves: evidences and assumptions
    for sub in ['evidences', 'assumptions']:
        d = items_dir / sub
        if not d.exists():
            continue
        for f in sorted(d.glob('*.md')):
            fm, body = load_markdown(f)
            if isinstance(fm, dict) and fm.get('__yaml_error'):
                errors.append((f, 'yaml_error', fm.get('__yaml_error')))
                continue
            validate_required_fields(fm, f, errors)
            # verify score fields are floats (report-only)
            check_score_is_float(fm, f, errors)
            enforce_level_matches_filename(fm, f, errors)
            # check for references or evidence
            has_ref = 'references' in fm and fm.get('references')
            has_evid = 'evidence' in fm and fm.get('evidence')
            if not has_ref and not has_evid:
                errors.append((f, 'missing_references_or_evidence'))
            # normalize references
            if 'references' in fm and isinstance(fm['references'], list):
                new_refs = []
                seen = set()
                for ref in fm['references']:
                    if not isinstance(ref, dict):
                        errors.append((f, 'bad_reference_format', ref))
                        continue
                    # ensure type
                    if 'type' not in ref:
                        # try to infer
                        if 'path' in ref:
                            ref['type'] = 'file'
                        else:
                            ref['type'] = 'file'
                    if 'path' in ref:
                        ref['path'] = normalize_path(str(ref['path']))
                        # validate existence
                        candidate = relative_in_repo(ref['path'], DEFAULT_IMPL_ROOT.parent.parent)
                        if not candidate.exists():
                            errors.append((f, 'ref_path_missing', ref['path']))
                    key = (ref.get('type'), ref.get('path') if 'path' in ref else None)
                    if key in seen:
                        continue
                    seen.add(key)
                    new_refs.append(ref)
                if args.apply and new_refs != fm['references']:
                    fm['references'] = new_refs
                    write_markdown(f, fm, body)
                    changes.append((f, 'normalize_references'))
            items_files[sub].append((f, fm, body))

    # Move concrete references from EXPECT/ASSERT to corresponding EVID under items/
    def move_concrete_refs_in_items():
        # For each expectation and assertion, find concrete refs and move them to EVID-L0-<n>
        for sub in ['expectations', 'assertions']:
            for (f, fm, body) in items_files.get(sub, []):
                refs = fm.get('references', []) if isinstance(fm.get('references'), list) else []
                concrete = [r for r in refs if is_concrete_ref(r)]
                if not concrete:
                    continue
                # determine L0 number from filename
                m = re.search(r'L0-(\d+)', f.name)
                if not m:
                    errors.append((f, 'cannot_determine_L0_from_name'))
                    continue
                n = m.group(1)
                evid_name = f'EVID-L0-{n}.md'
                evid_path = items_dir / 'evidences' / evid_name
                # Load or create EVID frontmatter
                if evid_path.exists():
                    evid_fm, evid_body = load_markdown(evid_path)
                    if isinstance(evid_fm, dict) and evid_fm.get('__yaml_error'):
                        errors.append((evid_path, 'yaml_error', evid_fm.get('__yaml_error')))
                        continue
                else:
                    evid_fm = {'id': f'EVID-L0-{n}', 'normative': False}
                    evid_body = ''
                # ensure evid has references list
                if 'references' not in evid_fm or not isinstance(evid_fm['references'], list):
                    evid_fm['references'] = []
                # move concrete refs (dedupe)
                moved = False
                for r in concrete:
                    # normalize path
                    if 'path' in r:
                        r['path'] = normalize_path(str(r['path']))
                    key = (r.get('type'), r.get('path'))
                    exists = any((er.get('type'), er.get('path')) == key for er in evid_fm['references'] if isinstance(er, dict))
                    if not exists:
                        evid_fm['references'].append(r)
                        moved = True
                    # remove from source fm references
                    try:
                        fm['references'].remove(r)
                    except ValueError:
                        pass
                if moved and args.apply:
                    # write evid
                    write_markdown(evid_path, evid_fm, evid_body)
                    changes.append((evid_path, 'add_concrete_refs'))
                    # write source file without concrete refs
                    write_markdown(f, fm, body)
                    changes.append((f, 'remove_concrete_refs'))
                elif moved:
                    changes.append((evid_path, 'would_add_concrete_refs'))
                    changes.append((f, 'would_remove_concrete_refs'))

    move_concrete_refs_in_items()

    # Sync canonical <-> items in a safe, normalized manner when apply
    if args.apply:
        for canonical_path, target_name in canonical_map.items():
            # infer prefix folder from canonical_path parent name
            prefix = canonical_path.parent.parent.name.lower()
            if 'expect' in prefix:
                sub = 'expectations'
            elif 'assert' in prefix:
                sub = 'assertions'
            elif 'assump' in prefix or 'assum' in prefix:
                sub = 'assumptions'
            elif 'evid' in prefix:
                sub = 'evidences'
            else:
                sub = prefix
            dst = items_dir / sub / target_name
            dst.parent.mkdir(parents=True, exist_ok=True)
            # Load canonical file, normalize it similarly to items normalization, then write both canonical and dst
            c_fm, c_body = load_markdown(canonical_path)
            if isinstance(c_fm, dict) and c_fm.get('__yaml_error'):
                errors.append((canonical_path, 'yaml_error', c_fm.get('__yaml_error')))
                # still attempt to copy raw bytes as fallback
                data = canonical_path.read_bytes()
                dst.write_bytes(data)
                changes.append((dst, 'sync_from_canonical_raw'))
                continue
            # verify scores in canonical are floats (report-only)
            check_score_is_float(c_fm, canonical_path, errors)
            # normalize: remove score from non-leaves
            if sub in ['expectations', 'assertions'] and 'score' in c_fm:
                c_fm.pop('score', None)
            # normalize references paths
            if 'references' in c_fm and isinstance(c_fm['references'], list):
                new_refs = []
                seen = set()
                for ref in c_fm['references']:
                    if not isinstance(ref, dict):
                        continue
                    if 'type' not in ref:
                        ref['type'] = 'file' if 'path' in ref else 'file'
                    if 'path' in ref:
                        ref['path'] = normalize_path(str(ref['path']))
                    key = (ref.get('type'), ref.get('path') if 'path' in ref else None)
                    if key in seen:
                        continue
                    seen.add(key)
                    new_refs.append(ref)
                c_fm['references'] = new_refs
            # write canonical normalized
            write_markdown(canonical_path, c_fm, c_body)
            changes.append((canonical_path, 'normalized_canonical'))
            # write same content to dst so trees match
            write_markdown(dst, c_fm, c_body)
            changes.append((dst, 'sync_from_canonical_normalized'))

    # report
    print('Proposed changes:')
    for c in changes:
        print('-', c)
    if errors:
        print('\nErrors/warnings:')
        for e in errors:
            print('-', e)
    else:
        print('\nNo errors detected.')
    # New pass: run trudag lint and auto-fix edges reported between non-normative items
    def run_trudag_and_fix_non_normative_edges():
        try:
            proc = subprocess.run(['trudag', 'manage', 'lint'], cwd=REPO_ROOT, capture_output=True, text=True)
        except FileNotFoundError:
            errors.append((REPO_ROOT, 'trudag_not_found', 'trudag CLI not found in PATH'))
            return
        out = proc.stdout + '\n' + proc.stderr
        pattern = re.compile(r'Graph contains edge (\S+) -> (\S+) between non-normative item\(s\)')
        matches = pattern.findall(out)
        if not matches:
            return
        # build quick index of canonical items by prefix and uid folder
        canonical_index = {}
        trudag_dir = root / '.trudag_items'
        for prefix_dir in sorted(trudag_dir.glob('*')):
            if not prefix_dir.is_dir():
                continue
            for uid_dir in sorted(prefix_dir.glob('*')):
                if not uid_dir.is_dir():
                    continue
                key = (prefix_dir.name.upper(), uid_dir.name.upper())
                # pick first md file in uid_dir (prefer named file)
                files = list(uid_dir.glob('*.md'))
                if files:
                    canonical_index[key] = files[0]
        for a, b in matches:
            # nodes look like PREFIX-UID e.g., ASSERTIONS-ASSERT_L0_1
            try:
                a_prefix, a_uid = a.split('-', 1)
                b_prefix, b_uid = b.split('-', 1)
            except Exception:
                errors.append((None, 'parse_trudag_node', (a, b)))
                continue
            # try to map to canonical path
            a_key = (a_prefix.upper(), a_uid.upper())
            b_key = (b_prefix.upper(), b_uid.upper())
            targets = []
            for key in (a_key, b_key):
                p = canonical_index.get(key)
                if p:
                    targets.append(p)
            # for each target found, if normative is False, suggest or apply flipping
            for p in targets:
                try:
                    fm, body = load_markdown(p)
                    if isinstance(fm, dict) and fm.get('__yaml_error'):
                        errors.append((p, 'yaml_error', fm.get('__yaml_error')))
                        continue
                    if fm.get('normative') is False:
                        if args.apply:
                            fm['normative'] = True
                            write_markdown(p, fm, body)
                            changes.append((p, 'auto_flip_normative_due_to_graph'))
                        else:
                            changes.append((p, 'would_auto_flip_normative_due_to_graph'))
                except Exception as e:
                    errors.append((p, 'auto_flip_error', str(e)))

    # run the auto-fix pass
    run_trudag_and_fix_non_normative = True
    if run_trudag_and_fix_non_normative:
        run_trudag_and_fix_non_normative_edges()

    if args.apply:
        print('\nApplied changes (see above).')
    else:
        print('\nDry-run complete. Re-run with --apply to make changes.')


if __name__ == '__main__':
    main()
