import pytest
import re
import click.testing
from pathlib import Path
import frontends.cli as cli
from trudag.error import ExitCodes
from tests.utils.project_files import simple_project_files
from trudag.utils import NEEDS_DEFAULT_FILENAME, DOTSTOP_DEFAULT_FILENAME
import logging

logger = logging.getLogger(__name__)


def add_score_to_evidence(evidence_name, runner):
    evidence_item = Path(f"./{evidence_name}.md")
    evidence_content = evidence_item.read_text()
    # TODO: annoying extra spaces added because project files contains those spaces. need to clean them up later.
    evidence_item.write_text(
        evidence_content.replace(
            "normative: true",
            f"normative: true            \n            references:\n                - type: file\n                  path: {evidence_name}.md\n            score:\n                 Testing: 0.5\n",
        )
    )
    assert runner.invoke(cli.main, ["manage", "set-item", "--links", evidence_name])


def add_validator_to_needs(item_names, runner):
    for item_name in item_names:
        item = Path(f"./{item_name}.md")
        item_content = item.read_text()
        # TODO: annoying extra spaces added because project files contains those spaces. need to clean them up later.
        item.write_text(
            item_content.replace(
                "normative: true",
                "normative: true            \n            evidence:\n                type: test_type\n                configuration:\n                        test_key: test_value\n",
            )
        )
        assert runner.invoke(cli.main, ["manage", "set-item", "--links", item_name])


def add_to_needs_file(project_path, item_names):
    needs_file = project_path / NEEDS_DEFAULT_FILENAME
    needs_file.touch(exist_ok=True)
    items_in_resolved = []
    for item_name in item_names:
        items_in_resolved.append(
            re.findall(
                rf'.*"{item_name}".*\[sha=.*\];',
                str(simple_project_files["dot_source"]),
            )[0]
        )
    items_in_resolved = "\n".join(items_in_resolved)
    needs_file.write_text(f"""
        digraph G {{
            {items_in_resolved}
        }}
    """)


def needs_files_checks(needs_dir, files):
    checks = []
    checks.append(needs_dir.exists())
    for f in files:
        checks.append((needs_dir / Path(f)).is_file())
    return checks


@pytest.mark.parametrize(
    "files",
    [simple_project_files],
    ids=["project files"],
)
def test_export_without_needs(project_workdir_git, monkeypatch):
    root_item = "ROOT-001"

    monkeypatch.chdir(project_workdir_git)
    runner = click.testing.CliRunner()

    result = runner.invoke(
        cli.main, ["export", "--artifact", "data.json", "-P", "test"]
    )
    assert result.exit_code == ExitCodes.SUCCESS


@pytest.mark.parametrize(
    "files",
    [simple_project_files],
    ids=["project files"],
)
def test_export_with_referenced_needs_fail(project_workdir_git, monkeypatch, caplog):
    root_item = "ROOT-001"
    needs_dir = Path("needs")
    needs_items = [
        "INTERMEDIATE-002",  # This item contains a reference and should fail to export
    ]

    monkeypatch.chdir(project_workdir_git)
    runner = click.testing.CliRunner()

    add_to_needs_file(project_workdir_git, needs_items)
    add_score_to_evidence("EVIDENCE-001", runner)

    result = runner.invoke(
        cli.main, ["export", "--artifact", "data.json", "-P", "test"]
    )
    assert result.exit_code != ExitCodes.SUCCESS
    assert (
        "frontends.cli.utils",
        logging.CRITICAL,
        "Needs item should not have references associated with it.",
    ) in caplog.record_tuples


@pytest.mark.parametrize(
    "files",
    [simple_project_files],
    ids=["project files"],
)
def test_export_with_validator_needs_fail(project_workdir_git, monkeypatch, caplog):
    root_item = "ROOT-001"
    needs_dir = Path("needs")
    needs_items = [
        "INTERMEDIATE-001",
    ]

    monkeypatch.chdir(project_workdir_git)
    runner = click.testing.CliRunner()

    add_validator_to_needs(needs_items, runner)
    add_to_needs_file(project_workdir_git, needs_items)
    add_score_to_evidence("EVIDENCE-001", runner)

    result = runner.invoke(
        cli.main, ["export", "--artifact", "data.json", "-P", "test"]
    )
    assert result.exit_code != ExitCodes.SUCCESS
    assert (
        "frontends.cli.utils",
        logging.CRITICAL,
        "Needs item should not have evidence or associated score.",
    ) in caplog.record_tuples


@pytest.mark.parametrize(
    "files",
    [simple_project_files],
    ids=["project files"],
)
def test_artifact_export_import_pass(project_workdir_git, monkeypatch):
    namespace = "testing"
    root_item = "ROOT-001"
    needs_dir = Path("needs")
    needs_items = ["INTERMEDIATE-001", "INTERMEDIATE-003"]

    monkeypatch.chdir(project_workdir_git)
    runner = click.testing.CliRunner()

    add_to_needs_file(project_workdir_git, needs_items)
    add_score_to_evidence("EVIDENCE-001", runner)

    result = runner.invoke(
        cli.main, ["export", "--artifact", "data.json", "-P", "test"]
    )

    assert result.exit_code == ExitCodes.SUCCESS

    result = runner.invoke(
        cli.main,
        [
            "import",
            "--artifact",
            "data.json",
            "-d",
            needs_dir.name,
            "-n",
            namespace,
            "-R",
            root_item,
        ],
    )
    assert result.exit_code == ExitCodes.SUCCESS
    assert any(
        needs_files_checks(
            needs_dir,
            [f"{namespace}.{root_item}.md"]
            + [f"{namespace}.{item}.md" for item in needs_items],
        )
    )
    assert runner.invoke(cli.main, ["publish", "-a"])


@pytest.mark.parametrize(
    "files",
    [simple_project_files],
    ids=["project files"],
)
def test_import_atomicity_on_fail(project_workdir_git, monkeypatch):
    namespace = "testing"
    root_item = "NON_EXISTENT"
    needs_dir = Path("needs")
    needs_items = ["INTERMEDIATE-001", "INTERMEDIATE-003"]

    monkeypatch.chdir(project_workdir_git)
    runner = click.testing.CliRunner()

    add_to_needs_file(project_workdir_git, needs_items)
    add_score_to_evidence("EVIDENCE-001", runner)

    original_dot_source = DOTSTOP_DEFAULT_FILENAME.read_text()
    result = runner.invoke(
        cli.main, ["export", "--artifact", "data.json", "-P", "test"]
    )

    assert result.exit_code == ExitCodes.SUCCESS

    result = runner.invoke(
        cli.main,
        [
            "import",
            "--artifact",
            "data.json",
            "-d",
            needs_dir.name,
            "-n",
            namespace,
            "-R",
            root_item,
        ],
    )
    assert result.exit_code != ExitCodes.SUCCESS
    # we should expect none of the needs files to have been created since we failed on import and also the dot file to be the same as before.
    assert not any(
        needs_files_checks(
            needs_dir,
            [f"{namespace}.{root_item}.md"]
            + [f"{namespace}.{item}.md" for item in needs_items],
        )
    )
    assert DOTSTOP_DEFAULT_FILENAME.read_text() == original_dot_source

    # we should still be able to run publish
    assert runner.invoke(cli.main, ["publish", "-a"])
